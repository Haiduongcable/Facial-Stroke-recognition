{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_alignment\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import cv2\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import sklearn \n",
    "import os \n",
    "import time \n",
    "from tqdm import tqdm\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import math\n",
    "import imutils\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "\n",
    "def imshow(img):\n",
    "    if len(img.shape) > 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "    else:\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "def imgshow(ax, img):\n",
    "    if len(img.shape) > 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "    else:\n",
    "        ax.imshow(img, cmap='gray')\n",
    "    \n",
    "def images_show(images):\n",
    "    fig = plt.figure(figsize=(15., 15.))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(4, 4),  # creates 2x2 grid of axes\n",
    "                     axes_pad=0.1,  # pad between axes in inch.\n",
    "                     )\n",
    "    for ax, im in zip(grid, images):\n",
    "        imgshow(ax, im)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/haiduong/anaconda3/envs/my_env/lib/python3.8/site-packages/mediapipe/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from typing import List, Tuple, Union\n",
    "import mediapipe as mp\n",
    "print(mp.__file__)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = \"/home/haiduong/Documents/VIN_BRAIN/Stroke_Recognition/Data_from_server/Data/val/positive\"\n",
    "\n",
    "\n",
    "\n",
    "file_list = []\n",
    "for file in os.listdir(path_folder):\n",
    "    file_list.append(path_folder +\"/\" +  file)\n",
    "\n",
    "\n",
    "folder_save_output = \"/home/haiduong/Documents/VIN_BRAIN/Stroke_Recognition/Data_from_server/Data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalized_to_pixel_coordinates(\n",
    "    normalized_x: float, normalized_y: float, image_width: int,\n",
    "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "    def is_valid_normalized_value(value: float) -> bool:\n",
    "        return (value > 0 or math.isclose(0, value)) and (value < 1 or\n",
    "                                                      math.isclose(1, value))\n",
    "\n",
    "    if not (is_valid_normalized_value(normalized_x) and\n",
    "          is_valid_normalized_value(normalized_y)):\n",
    "        return None\n",
    "    \n",
    "    x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "    y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "    return x_px, y_px\n",
    "\n",
    "def get_keypoint(landmark_list, image):\n",
    "    if not landmark_list:\n",
    "        return\n",
    "    if image.shape[2] != 3:\n",
    "        raise ValueError('Input image must contain three channel rgb data.')\n",
    "    image_rows, image_cols, _ = image.shape\n",
    "    idx_to_coordinates = {}\n",
    "    for idx, landmark in enumerate(landmark_list.landmark):\n",
    "        if ((landmark.HasField('visibility') and\n",
    "         landmark.visibility < VISIBILITY_THRESHOLD) or\n",
    "        (landmark.HasField('presence') and\n",
    "         landmark.presence < PRESENCE_THRESHOLD)):\n",
    "            continue\n",
    "        landmark_px = _normalized_to_pixel_coordinates(landmark.x, landmark.y,\n",
    "                                                   image_cols, image_rows)\n",
    "        if landmark_px:\n",
    "            idx_to_coordinates[idx] = landmark_px\n",
    "    l_keypoint = []\n",
    "    for landmark_px in idx_to_coordinates.values():\n",
    "        l_keypoint.append(landmark_px)\n",
    "    return l_keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found (467, 2)\n",
      "Found (442, 2)\n",
      "Found (458, 2)\n",
      "Found (454, 2)\n",
      "Found (466, 2)\n",
      "Found (466, 2)\n",
      "Found (462, 2)\n",
      "Found (462, 2)\n",
      "Found (457, 2)\n",
      "Found (466, 2)\n",
      "Found (465, 2)\n",
      "Found (467, 2)\n",
      "Found (467, 2)\n",
      "Found (442, 2)\n",
      "Found (458, 2)\n",
      "Found (451, 2)\n",
      "Found (466, 2)\n",
      "Found (462, 2)\n",
      "Found (461, 2)\n",
      "Found (467, 2)\n",
      "Found (461, 2)\n",
      "Found (449, 2)\n",
      "Found (454, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# For static images:\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "    \n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5)\n",
    "\n",
    "for idx, file in enumerate(file_list):\n",
    "    name_image = file.split(\"/\")[-1]\n",
    "    image = cv2.imread(file)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Print and draw face mesh landmarks on the image.\n",
    "    if not results.multi_face_landmarks:\n",
    "        continue\n",
    "    annotated_image = np.zeros_like(image, dtype = np.uint8) + 255\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        l_keypoint = get_keypoint(face_landmarks, annotated_image)\n",
    "#         if (np.shape(l_keypoint)[0] != 468):\n",
    "#             print(\"Found\", np.shape(l_keypoint))\n",
    "        mp_drawing.draw_landmarks(\n",
    "          image=annotated_image,\n",
    "          landmark_list=face_landmarks,\n",
    "          landmark_drawing_spec=drawing_spec)\n",
    "        print(rs)\n",
    "#     cv2.imwrite(folder_save_output + \"/\" + name_image, annotated_image)\n",
    "\n",
    "\n",
    "# drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# with mp_face_mesh.FaceMesh(\n",
    "#     min_detection_confidence=0.5,\n",
    "#     min_tracking_confidence=0.5) as face_mesh:\n",
    "#   while cap.isOpened():\n",
    "#     success, image = cap.read()\n",
    "#     if not success:\n",
    "#       print(\"Ignoring empty camera frame.\")\n",
    "#       # If loading a video, use 'break' instead of 'continue'.\n",
    "#       continue\n",
    "\n",
    "#     # Flip the image horizontally for a later selfie-view display, and convert\n",
    "#     # the BGR image to RGB.\n",
    "#     image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "#     # To improve performance, optionally mark the image as not writeable to\n",
    "#     # pass by reference.\n",
    "#     image.flags.writeable = False\n",
    "#     results = face_mesh.process(image)\n",
    "\n",
    "#     # Draw the face mesh annotations on the image.\n",
    "#     image.flags.writeable = True\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "#     if results.multi_face_landmarks:\n",
    "#       for face_landmarks in results.multi_face_landmarks:\n",
    "#         mp_drawing.draw_landmarks(\n",
    "#             image=image,\n",
    "#             landmark_list=face_landmarks,\n",
    "#             connections=mp_face_mesh.FACE_CONNECTIONS,\n",
    "#             landmark_drawing_spec=drawing_spec,\n",
    "#             connection_drawing_spec=drawing_spec)\n",
    "#     cv2.imshow('MediaPipe FaceMesh', image)\n",
    "#     if cv2.waitKey(5) & 0xFF == 27:\n",
    "#       break\n",
    "# cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "folder_mesh = \"/home/haiduong/Documents/VIN_BRAIN/Stroke_Recognition/Method/face-emotion-recognition/output_mesh\"\n",
    "for file in os.listdir(folder_mesh):\n",
    "    image_mesh = cv2.imread(folder_mesh + \"/\" + file)\n",
    "#     imshow(image_mesh)\n",
    "#     convert_mesh = np.where(image_mesh, image_mesh = [])\n",
    "    print(image_mesh)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/haiduong/Documents/VIN_BRAIN/Stroke_Recognition/Data/train/positive\"\n",
    "des_f = \"/home/haiduong/Documents/VIN_BRAIN/Stroke_Recognition/Data/train/log\"\n",
    "l_n_swap_face = []\n",
    "for file in os.listdir(path):\n",
    "    if \"to\" not in file:\n",
    "        l_n_swap_face.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645\n"
     ]
    }
   ],
   "source": [
    "print(len(l_n_swap_face))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in l_n_swap_face:\n",
    "#     source = path + \"/\" + file\n",
    "#     des = des_f + \"/\" + file\n",
    "#     shutil.copy(source, des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert Dataset\n",
    "path_positive = \"/home/haiduong/Documents/VIN_BRAIN/Stroke_Recognition/Data/val/negative\"\n",
    "# #path_save = \"/home/haiduong/Documents/VIN_BRAIN/Stroke_Recognition/Data/val_landmark_image/convert_positive\"\n",
    "# Training_60182561.jpg\n",
    "# l_extend = []\n",
    "# for file in os.listdir(path_positive):\n",
    "#     name = file.split(\"/\")[-1]\n",
    "#     extend = name.split(\".\")[-1]\n",
    "#     if extend not in l_extend:\n",
    "#         l_extend.append(extend)\n",
    "#     image = cv2.imread(path_positive + \"/\" + file)\n",
    "#     imshow(image)\n",
    "# #     if len(np.shape(image)) < 3:\n",
    "# #         print(file)\n",
    "# #     elif np.shape(image)[2] != 3:\n",
    "# #         print(file)\n",
    "# #     if len(extend) == 3:\n",
    "# #         name_save = name[:-3] + \"jpg\"\n",
    "# #     elif len(extend) == 4:\n",
    "# #         name_save = name[:-4] + \"jpg\"\n",
    "# #     cv2.imwrite(path_save +\"/\" + name_save, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "path = path_positive + \"/\" + \"Training_60182561.jpg\"\n",
    "image = cv2.imread(path)\n",
    "# image = io.imread(path)\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jpg']\n"
     ]
    }
   ],
   "source": [
    "print(l_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
